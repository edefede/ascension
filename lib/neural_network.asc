// ============================================================================
// NEURAL NETWORK LIBRARY for Ascension
// ============================================================================
// Versione: 1.0
// Autore: EdeFede + Claude
// Richiede: Ascension 12.7+ (Math Edition)
// ============================================================================
// Contenuto:
//   1. Funzioni di attivazione (sigmoid, relu, step)
//   2. Singolo Neurone
//   3. Perceptron (single layer per AND/OR/NAND/NOR)
//   4. Multi-Layer Perceptron per XOR
// ============================================================================

// ============================================================================
// SEZIONE 1: FUNZIONI DI ATTIVAZIONE
// ============================================================================

// Sigmoid: output tra 0 e 1, smooth
// Usata per probabilità e output continui
func sigmoid(x) {
    return 1.0 / (1.0 + exp(0 - x));
}

// Derivata della sigmoid (per backpropagation)
// Se y = sigmoid(x), allora y' = y * (1 - y)
func sigmoid_derivative(sigmoid_output) {
    return sigmoid_output * (1.0 - sigmoid_output);
}

// ReLU: Rectified Linear Unit
// Popolare nelle reti moderne, veloce da calcolare
func relu(x) {
    if (x > 0) {
        return x;
    }
    return 0;
}

// Derivata ReLU
func relu_derivative(x) {
    if (x > 0) {
        return 1;
    }
    return 0;
}

// Step function: output binario 0 o 1
// Usata nel perceptron classico
func step(x) {
    if (x >= 0) {
        return 1;
    }
    return 0;
}

// Tanh: output tra -1 e 1
func tanh_activation(x) {
    ep = exp(x);
    en = exp(0 - x);
    return (ep - en) / (ep + en);
}

// ============================================================================
// SEZIONE 2: SINGOLO NEURONE
// ============================================================================
// Un neurone calcola: output = activation(sum(inputs * weights) + bias)

// Neurone con 2 input (il caso più comune per iniziare)
// w1, w2 = pesi, b = bias, x1, x2 = input
func neuron_2in(x1, x2, w1, w2, b) {
    sum = x1 * w1 + x2 * w2 + b;
    return sigmoid(sum);
}

// Neurone con 3 input
func neuron_3in(x1, x2, x3, w1, w2, w3, b) {
    sum = x1 * w1 + x2 * w2 + x3 * w3 + b;
    return sigmoid(sum);
}

// Neurone con step function (perceptron classico)
func neuron_step(x1, x2, w1, w2, b) {
    sum = x1 * w1 + x2 * w2 + b;
    return step(sum);
}

// ============================================================================
// SEZIONE 3: PERCEPTRON - Single Layer
// ============================================================================
// Il Perceptron può imparare funzioni linearmente separabili: AND, OR, NAND, NOR
// NON può imparare XOR (richiede hidden layer)

// Inizializza pesi random tra -1 e 1
func random_weight() {
    return random() * 2 - 1;
}

// Inizializza peso random piccolo (migliore per training)
func random_weight_small() {
    return random() * 0.5 - 0.25;
}

// --- PERCEPTRON TRAINING ---
// Algoritmo: w_new = w_old + learning_rate * error * input
// dove error = target - output

// Training step singolo per perceptron con 2 input
// Ritorna l'errore assoluto
func perceptron_train_step(x1, x2, target, lr) {
    // I pesi sono in variabili globali: p_w1, p_w2, p_bias
    
    // Forward pass
    sum = x1 * p_w1 + x2 * p_w2 + p_bias;
    output = step(sum);
    
    // Calcola errore
    error = target - output;
    
    // Aggiorna pesi (Perceptron Learning Rule)
    p_w1 = p_w1 + lr * error * x1;
    p_w2 = p_w2 + lr * error * x2;
    p_bias = p_bias + lr * error;
    
    return abs(error);
}

// Predizione del perceptron (dopo training)
func perceptron_predict(x1, x2) {
    sum = x1 * p_w1 + x2 * p_w2 + p_bias;
    return step(sum);
}

// ============================================================================
// SEZIONE 4: MULTI-LAYER PERCEPTRON per XOR
// ============================================================================
// XOR richiede almeno un hidden layer con 2 neuroni
// Architettura: 2 input -> 2 hidden (sigmoid) -> 1 output (sigmoid)
//
// Pesi:
//   Hidden layer: wh[0][0], wh[0][1] (neurone h0)
//                 wh[1][0], wh[1][1] (neurone h1)
//                 bh[0], bh[1] (bias hidden)
//   Output layer: wo[0], wo[1] (da hidden a output)
//                 bo (bias output)

// Variabili globali per MLP (inizializzate da mlp_init)
// Hidden layer weights: mlp_wh (matrice 2x2)
// Hidden layer bias: mlp_bh (array 2)
// Output layer weights: mlp_wo (array 2)  
// Output layer bias: mlp_bo (scalare)

// Inizializza la rete MLP per XOR
// Usa variabili globali scalari per evitare problemi di scope
func mlp_init() {
    // Hidden layer weights (2 neuroni, 2 input ciascuno)
    // Pesi più ampi per rompere simmetria
    // Neurone h0
    global mlp_wh00 = random() * 2 - 1;
    global mlp_wh01 = random() * 2 - 1;
    // Neurone h1
    global mlp_wh10 = random() * 2 - 1;
    global mlp_wh11 = random() * 2 - 1;
    
    // Hidden layer bias (piccoli)
    global mlp_bh0 = random() * 0.5 - 0.25;
    global mlp_bh1 = random() * 0.5 - 0.25;
    
    // Output layer weights
    global mlp_wo0 = random() * 2 - 1;
    global mlp_wo1 = random() * 2 - 1;
    
    // Output bias
    global mlp_bo = random() * 0.5 - 0.25;
}

// Forward pass della rete MLP
// Ritorna l'output e salva le attivazioni per backprop
func mlp_forward(x1, x2) {
    // Salva input per backprop
    global mlp_x1 = x1;
    global mlp_x2 = x2;
    
    // Hidden layer
    h0_sum = x1 * mlp_wh00 + x2 * mlp_wh01 + mlp_bh0;
    h1_sum = x1 * mlp_wh10 + x2 * mlp_wh11 + mlp_bh1;
    
    global mlp_h0 = sigmoid(h0_sum);
    global mlp_h1 = sigmoid(h1_sum);
    
    // Output layer
    o_sum = mlp_h0 * mlp_wo0 + mlp_h1 * mlp_wo1 + mlp_bo;
    global mlp_out = sigmoid(o_sum);
    
    return mlp_out;
}

// Backpropagation e aggiornamento pesi
// Ritorna l'errore quadratico
func mlp_backward(target, lr) {
    // Errore output
    output_error = target - mlp_out;
    
    // Delta output (derivata sigmoid)
    output_delta = output_error * sigmoid_derivative(mlp_out);
    
    // Errore propagato ai neuroni hidden
    h0_error = output_delta * mlp_wo0;
    h1_error = output_delta * mlp_wo1;
    
    // Delta hidden
    h0_delta = h0_error * sigmoid_derivative(mlp_h0);
    h1_delta = h1_error * sigmoid_derivative(mlp_h1);
    
    // --- Aggiorna pesi output layer ---
    global mlp_wo0 = mlp_wo0 + lr * output_delta * mlp_h0;
    global mlp_wo1 = mlp_wo1 + lr * output_delta * mlp_h1;
    global mlp_bo = mlp_bo + lr * output_delta;
    
    // --- Aggiorna pesi hidden layer ---
    global mlp_wh00 = mlp_wh00 + lr * h0_delta * mlp_x1;
    global mlp_wh01 = mlp_wh01 + lr * h0_delta * mlp_x2;
    global mlp_bh0 = mlp_bh0 + lr * h0_delta;
    
    global mlp_wh10 = mlp_wh10 + lr * h1_delta * mlp_x1;
    global mlp_wh11 = mlp_wh11 + lr * h1_delta * mlp_x2;
    global mlp_bh1 = mlp_bh1 + lr * h1_delta;
    
    // Ritorna errore quadratico
    return output_error * output_error;
}

// Training completo su un esempio
func mlp_train_step(x1, x2, target, lr) {
    mlp_forward(x1, x2);
    return mlp_backward(target, lr);
}

// Predizione MLP (forward senza salvare stato)
func mlp_predict(x1, x2) {
    h0 = sigmoid(x1 * mlp_wh00 + x2 * mlp_wh01 + mlp_bh0);
    h1 = sigmoid(x1 * mlp_wh10 + x2 * mlp_wh11 + mlp_bh1);
    out = sigmoid(h0 * mlp_wo0 + h1 * mlp_wo1 + mlp_bo);
    return out;
}

// Predizione con soglia (per output binario)
func mlp_predict_binary(x1, x2) {
    out = mlp_predict(x1, x2);
    if (out >= 0.5) {
        return 1;
    }
    return 0;
}

// ============================================================================
// SEZIONE 5: UTILITY
// ============================================================================

// Mean Squared Error per un dataset
// (da implementare con array di esempi)

// Stampa stato della rete MLP
func mlp_print_weights() {
    print("=== MLP Weights ===");
    print("Hidden layer:");
    print("  h0: w0=", mlp_wh00, "w1=", mlp_wh01, "b=", mlp_bh0);
    print("  h1: w0=", mlp_wh10, "w1=", mlp_wh11, "b=", mlp_bh1);
    print("Output layer:");
    print("  wo0=", mlp_wo0, "wo1=", mlp_wo1, "b=", mlp_bo);
}

